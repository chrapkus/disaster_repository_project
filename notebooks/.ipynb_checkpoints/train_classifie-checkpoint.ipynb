{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb89a4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SzymonGrzebyta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SzymonGrzebyta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SzymonGrzebyta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import multioutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c967161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///../disaster.db')\n",
    "df = pd.read_sql(\"SELECT * FROM disaster\", engine)\n",
    "X = df['message']\n",
    "Y = df.drop(['id', 'message', 'original', 'genre'], axis = 1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b2f6d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # lower all text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Normalize text - removing punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    # Tokenize -> slit into list\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words_list = stopwords.words('english')\n",
    "    tokens = [token.strip() for token in tokens if token not in stop_words_list]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmed = [WordNetLemmatizer().lemmatize(token, pos = 'v') for token in tokens]\n",
    "    \n",
    "    # Steaming\n",
    "    stemmed = [PorterStemmer().stem(word) for word in lemmed]\n",
    "\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b004321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    #setting pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', multioutput.MultiOutputClassifier (RandomForestClassifier()))\n",
    "        ])\n",
    "\n",
    "    # fbeta_score scoring object using make_scorer()\n",
    "    scorer = 'f1_weighted'\n",
    "\n",
    "    #model parameters for GridSearchCV\n",
    "    parameters = {  'vect__max_df': (0.3, 0.5),\n",
    "                    'clf__estimator__n_estimators': [10, 5],\n",
    "                    'clf__estimator__min_samples_split': [2, 10]\n",
    "              }\n",
    "    cv = GridSearchCV(pipeline, param_grid= parameters, scoring = scorer, verbose =7, cv = 2)\n",
    "\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cd4b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "import numpy as np\n",
    "def eval_metrics(ArrayL, ArrayP, col_names):\n",
    "    \"\"\"Evalute metrics of the ML pipeline model\n",
    "    \n",
    "    inputs:\n",
    "    ArrayL: array. Array containing the real labels.\n",
    "    ArrayP: array. Array containing predicted labels.\n",
    "    col_names: list of strings. List containing names for each of the ArrayP fields.\n",
    "       \n",
    "    Returns:\n",
    "    data_metrics: Contains accuracy, precision, recall \n",
    "    and f1 score for a given set of ArrayL and ArrayP labels.\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    \n",
    "    # Evaluate metrics for each set of labels\n",
    "    for i in range(len(col_names)):\n",
    "        accuracy = accuracy_score(ArrayL[:, i], ArrayP[:, i])\n",
    "        precision = precision_score(ArrayL[:, i], ArrayP[:, i])\n",
    "        recall = recall_score(ArrayL[:, i], ArrayP[:, i])\n",
    "        f1 = f1_score(ArrayL[:, i], ArrayP[:, i])\n",
    "        \n",
    "        metrics.append([accuracy, precision, recall, f1])\n",
    "    \n",
    "    # store metrics\n",
    "    metrics = np.array(metrics)\n",
    "    data_metrics = pd.DataFrame(data = metrics, index = col_names, columns = ['Accuracy', 'Precision', 'Recall', 'F1'])\n",
    "      \n",
    "    return data_metrics    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f3c5e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      "[CV 1/2] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__max_df=0.3;, score=0.532 total time=  36.4s\n",
      "[CV 2/2] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__max_df=0.3;, score=0.527 total time=  32.3s\n",
      "[CV 1/2] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__max_df=0.5;, score=0.521 total time=  30.8s\n",
      "[CV 2/2] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__max_df=0.5;, score=0.529 total time=  30.1s\n",
      "[CV 1/2] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=5, vect__max_df=0.3;, score=0.557 total time=  24.9s\n",
      "[CV 2/2] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=5, vect__max_df=0.3;, score=0.551 total time=  25.3s\n",
      "[CV 1/2] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=5, vect__max_df=0.5;, score=0.546 total time=  24.4s\n",
      "[CV 2/2] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=5, vect__max_df=0.5;, score=0.554 total time=  24.0s\n",
      "[CV 1/2] END clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, vect__max_df=0.3;, score=0.557 total time=  28.3s\n",
      "[CV 2/2] END clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, vect__max_df=0.3;, score=0.565 total time=  27.9s\n",
      "[CV 1/2] END clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, vect__max_df=0.5;, score=0.560 total time=  27.9s\n",
      "[CV 2/2] END clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, vect__max_df=0.5;, score=0.565 total time=  27.4s\n",
      "[CV 1/2] END clf__estimator__min_samples_split=10, clf__estimator__n_estimators=5, vect__max_df=0.3;, score=0.551 total time=  23.1s\n",
      "[CV 2/2] END clf__estimator__min_samples_split=10, clf__estimator__n_estimators=5, vect__max_df=0.3;, score=0.551 total time=  22.6s\n",
      "[CV 1/2] END clf__estimator__min_samples_split=10, clf__estimator__n_estimators=5, vect__max_df=0.5;, score=0.557 total time=  22.9s\n",
      "[CV 2/2] END clf__estimator__min_samples_split=10, clf__estimator__n_estimators=5, vect__max_df=0.5;, score=0.550 total time=  22.7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        CountVectorizer(tokenizer=<function tokenize at 0x0000016FFB1C4AF0>)),\n",
       "                                       ('tfidf', TfidfTransformer()),\n",
       "                                       ('clf',\n",
       "                                        MultiOutputClassifier(estimator=RandomForestClassifier()))]),\n",
       "             param_grid={'clf__estimator__min_samples_split': [2, 10],\n",
       "                         'clf__estimator__n_estimators': [10, 5],\n",
       "                         'vect__max_df': (0.3, 0.5)},\n",
       "             scoring='f1_weighted', verbose=7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model()\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c7a9529",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "add334a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-4a5726aad16f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcol_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0meval_metrics0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_metrics0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "col_names = list(Y.columns.values)\n",
    "eval_metrics0 = eval_metrics(np.array(Y_test), Y_pred, col_names)\n",
    "print(eval_metrics0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f87a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_results(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4b87c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mess =np.array(['We are starving please help'])\n",
    "sol = model.predict(mess)\n",
    "names = list(Y.columns)\n",
    "for index,x in enumerate(sol[0]):\n",
    "    if x == 1:\n",
    "        print(names[index])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
